# Setting up the database

- Install postgres
- Create a database named 'sociallists'
- In python, do:

  >>> from sociallists import db
  >>> db.Base.metadata.create_all()

# Some notes on asynchrony

I spent some time trying to convert this codebase to asyncio so that feed
updates can be SUPER PARALLEL. What I discovered is that if I do this the HTTP
stack begins to misbehave and timeout and stuff, and I'm not entirely sure why.

So aiohttp doesn't seem to be a good choice for me.

Using coroutines but a fundamentally blocking HTTP stack (asyncio + requests)
doesn't actually buy me anything because all my threads will be blocked on the
same fixed number of HTTP requests anyway, so it will be a lot of complexity
for absolutely no efficiency gain.

(In addition, the async/await stuff is currently very ugly and the docs are
complicated and I just like this one better for now. So we're sticking with
the slow but useful. Rewriting the feed update process to be massively parallel
is still something I want to do but it will have to wait.)

# TODO:

UI:
- Fill out the More box in the UI to expand things
- Disable images in feeds as an option.
- Different river layouts (normal, big images, all images...)
- Progress bars
- Error boxes

Shipping:
- Re-visit use of requirements and packages and vendoring and the like

Feed Stuff:
- Extract enclosures and the like
- Put river items in the DB so that we can efficiently re-run the processing
  steps for items that are out of the feed
- For the future, we'll probably need options on individual feeds for
  processing to make them better. It is possible to over-engineer this so be
  careful.
- Implement ATOM cache timeout and RSS skip hours and the like to poll more
  intelligently.
- Implement pubsubhubbub
